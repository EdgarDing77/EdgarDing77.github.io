# 日志中心

## ELK

### Overview

Logstash和Beats进行数据收集、聚合和存储进Elasticsearch，Kibana允许我们对数据进行交互式探索、可视化和监控，Elasticsearch是进行索引、搜索和分析的地方。

组件分工：

- filebeat：部署在每台应用服务器、数据库、中间件中，负责日志抓取与日志聚合
  - 日志聚合：把多行日志合并成一条，例如exception的堆栈信息等
- logstash：通过各种filter结构化日志信息，并把字段transform成对应的类型
- elasticsearch：负责存储和查询日志信息
- kibana：通过ui展示日志信息、还能生成饼图、柱状图等

![img](https://cdn.jsdelivr.net/gh/edgarding77/microservice-platform-doc@latest/image/func/elk_overview.png)

### 使用说明

- 5601:Kibana web interface
- 9200:Elasticsearch JSON interface
- 9300:Elasticsearch transport interface
- 5044:Logstash Beats interface

- 9600:Logstash API endpoint

结构化日志：

```json
{
  "timestamp": "时间",
  "message": "具体日志信息",
  "threadName": "线程名",
  "serverPort": "服务端口",
  "serverIp": "服务ip",
  "logLevel": "日志级别",
  "appName": "工程名称",
  "classname": "类名"
}
```

- 5601 (Kibana web interface).
- 9200 (Elasticsearch JSON interface).
- 5044 (Logstash Beats interface, receives logs from Beats such as Filebeat – see the *[Forwarding logs with Filebeat](https://elk-docker.readthedocs.io/#forwarding-logs-filebeat)* section).

## Docker部署

> **注意，确保内存足够，docker需要4g内存，其中es需要分配2g，不然无法启动。**

环境准备：

- docker version: 1.13.1

- ELK：`latest`, `7.16.3`: ELK 7.16.3.
- filebeats：7.17

部署参考：

- https://elk-docker.readthedocs.io/#prerequisites
- https://blog.csdn.net/m0_37063785/article/details/101074010

### ELK部署

#### 1.修改mmap计数大于等于262144的限制

> ***max_map_count\***文件包含限制一个进程可以拥有的VMA(虚拟内存区域)的数量。虚拟内存区域是一个连续的虚拟地址空间区域。在进程的生命周期中，每当程序尝试在内存中映射文件，链接到共享内存段，或者分配堆空间的时候，这些区域将被创建。调优这个值将限制进程可拥有VMA的数量。限制一个进程拥有VMA的总数可能导致应用程序出错，因为当进程达到了VMA上线但又只能释放少量的内存给其他的内核进程使用时，操作系统会抛出内存不足的错误。如果你的操作系统在NORMAL区域仅占用少量的内存，那么调低这个值可以帮助释放内存给内核用。

```bash
#在/etc/sysctl.conf文件最后添加一行
vm.max_map_count=655360
#并执行命令
sysctl -p
```

- 这里如果使用docker for mac，[参考这里](https://elk-docker.readthedocs.io/#overriding-variables)

#### 2.下载运行镜像

```bash
docker run -p 5601:5601 -p 9200:9200 -p 9300:9300 -p 5044:5044 --name elk -d sebp/elk
```

#### 3.准备elasticsearch的配置文件

```bash
mkdir /opt/elk/elasticsearch/conf
#复制elasticsearch的配置出来
docker cp elk:/etc/elasticsearch/elasticsearch.yml ~/elk/elasticsearch/conf
```

#### 4.准备logstash的配置文件

```bash
mkdir /opt/elk/logstash/conf
#复制logstash的配置出来
docker cp elk:/etc/logstash/conf.d/. ~/elk/logstash/conf/
```

#### 5.准备logstash的patterns文件

```bash
mkdir /opt/elk/logstash/patterns
# 新建一个java的patterns文件，vim java.patterns 内容如下
# user-center
MYAPPNAME ([0-9a-zA-Z_-]*)
# RMI TCP Connection(2)-127.0.0.1
MYTHREADNAME ([0-9a-zA-Z._-]|\(|\)|\s)*
```

- 就是一个名字叫为java的文件。

#### 6.删除02-beats-input.conf的最后三句，去掉强制认证（这里注意）

`vim ~/elk/logstash/conf/02-beats-input.conf`

```conf
client_inactivity_timeout => 36000 # 添加
#ssl => true 
#ssl_certificate => "/pki/tls/certs/logstash.crt"
#ssl_key => "/pki/tls/private/logstash.key"
```

#### 7.修改10-syslog.conf配置

`vim ~/elk/logstash/conf/10-syslog.conf `

```conf
filter {
  if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{host}" ]
    }
    syslog_pri { }
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }
  if [fields][docType] == "sys-log" {
    grok {
      patterns_dir => ["/opt/elk/logstash/patterns"]
      match => { "message" => "\[%{NOTSPACE:appName}:%{NOTSPACE:serverIp}:%{NOTSPACE:serverPort}\] %{TIMESTAMP_ISO8601:logTime} %{LOGLEVEL:logLevel} %{WORD:pid} \[%{MYAPPNAME:traceId}\] \[%{MYTHREADNAME:threadName}\] %{NOTSPACE:classname} %{GREEDYDATA:message}" }
      overwrite => ["message"]
    }
    date {
      match => ["logTime","yyyy-MM-dd HH:mm:ss.SSS Z"]
    }
    date {
      match => ["logTime","yyyy-MM-dd HH:mm:ss.SSS"]
      target => "timestamp"
      locale => "en"
      timezone => "+08:00"
    }
    mutate {  
      remove_field => "logTime"
      remove_field => "@version"
      remove_field => "host"
      remove_field => "offset"
    }
  }
  if [fields][docType] == "point-log" {
    grok {
      patterns_dir => ["/opt/elk/logstash/patterns"]
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:logTime}\|%{MYAPPNAME:appName}\|%{WORD:resouceid}\|%{MYAPPNAME:type}\|%{GREEDYDATA:object}"
      }
    }
    kv {
        source => "object"
        field_split => "&"
        value_split => "="
    }
    date {
      match => ["logTime","yyyy-MM-dd HH:mm:ss.SSS Z"]
    }
    date {
      match => ["logTime","yyyy-MM-dd HH:mm:ss.SSS"]
      target => "timestamp"
      locale => "en"
      timezone => "+08:00"
    }
    mutate {
      remove_field => "logTime"
      remove_field => "@version"
      remove_field => "host"
      remove_field => "offset"
    }
  }
}
```

- 以上logstash结构化配置是以当前项目为日志格式配置

#### 8.修改30-output.conf配置

```conf
output {
  if [fields][docType] == "sys-log" {
    elasticsearch {
      hosts => ["localhost"]
      manage_template => false
      index => "sys-log-%{+YYYY.MM.dd}"
      document_type => "%{[@metadata][type]}"
    }
  }
  if [fields][docType] == "point-log" {
    elasticsearch {
      hosts => ["localhost"]
      manage_template => false
      index => "point-log-%{+YYYY.MM.dd}"
      document_type => "%{[@metadata][type]}"
      routing => "%{type}"
    }
  }
}
```

- document_type配置方式废除，[具体参考](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/removal-of-types.html)

#### 9.创建运行脚本

`vim elk/start.sh`

```bash
docker stop elk
docker rm elk

docker run -p 5601:5601 -p 9200:9200 -p 9300:9300 -p 5044:5044 \
    -e LS_HEAP_SIZE="1g" -e ES_JAVA_OPTS="-Xms2g -Xmx2g" \
    -v ~/elk/elasticsearch/data:/var/lib/elasticsearch \
    -v ~/elk/elasticsearch/plugins:/opt/elasticsearch/plugins \
    -v ~/elk/elasticsearch/conf/elasticsearch.yml:/etc/elasticsearch/elasticsearch.yml \
    #-v ~/elk/elasticsearch/log:/var/log/elasticsearch \
    -v ~/elk/logstash/conf:/etc/logstash/conf.d \
    -v ~/elk/logstash/patterns:/opt/logstash/patterns \
    #-v ~/elk/logstash/log:/var/log/logstash \
    --name elk \
    -d sebp/elk
```

启动：`sh start.sh`

### Filebeat部署

#### 直接部署

1. 从[官网](https://www.elastic.co/cn/downloads/beats/filebeat)下载

2. 修改配置文件filebeat.yml

   1. 修改filebeat.inputs 为以下内容，其中paths为项目的日志路径

   ```yml
   filebeat.inputs:
   - type: log
     enabled: true
     paths:
       - /Users/edgarding/edgarding_ideaProject/logs/application/*/*.log
     exclude_lines: ['\sDEBUG\s\d']
     exclude_files: ['sc-admin.*.log$']
     fields:
       docType: sys-log
       project: microservice-platform
     multiline:
       pattern: '^\[\S+:\S+:\d{2,}] '
       negate: true
       match: after
   - type: log
     enabled: true
     paths:
       - /Users/edgarding/edgarding_ideaProject/logs/point/*.log
     fields:
       docType: point-log
       project: microservice-platform
   ```

   2. 修改output.logstash为以下内容，其中hosts为logstash的部署地址

   ```yml
   output.logstash:
   	hosts: ["localhost:5044"]
   # bulk_max_size: 2048
   # output.elasticsearch:
   	# hosts: ["localhost:9200"]
   ```

3. 启动filebeat

   ```bash
   ./filebeat modules enable logstash
   
   ./filebeat setup --index-management -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["localhost:9200"]'
   
   ./filebeat -c filebeat.yml -e
   ```

配置参考：https://www.elastic.co/guide/en/beats/filebeat/7.17/configuring-howto-filebeat.html

input配置：https://www.elastic.co/guide/en/beats/filebeat/7.17/configuration-filebeat-options.html

#### docker部署

pwd -> elk

```bash
docker pull docker.elastic.co/beats/filebeat:7.17.0
```

```bash
docker run -d \
  --name=filebeat \
  --user=root \
  --volume="$(pwd)/docker/filebeat.docker.yml:/usr/share/filebeat/filebeat.yml:ro" \
  --volume="/var/lib/docker/containers:/var/lib/docker/containers:ro" \
  --volume="/var/run/docker.sock:/var/run/docker.sock:ro" \
  docker.elastic.co/beats/filebeat:7.17.0 filebeat -e -strict.perms=false \
  -E output.elasticsearch.hosts=["elk:9200"] \
  -- link=elk:elk
```



### 部署问题

> 启动脚本报错：`container_linux.go:235: starting container process caused "container init exited prematurely"`

在elasticsearch下创建log、plugins、data三个文件夹。

> 错误提示：`cat: /var/log/elasticsearch/elasticsearch.log: No such file or directory`

解决参考：

- https://blog.csdn.net/qq_38865022/article/details/115706795
- https://elk-docker.readthedocs.io/#es-not-starting-not-enough-memory

问题原因：没有足够的内存，Elasticsearch需要至少2GB内存，而Kibana、logstash其它服务也会使用内存，所以至少4GB。

解决方案：MAC端修改resource限制

> 错误提示：`Exiting: error unpacking config data: more than one namespace configured accessing 'output' (source:'filebeat.yml')`



> 错误提示：`Failed to publish events caused by: client is not connected`

解决方案：https://discuss.elastic.co/t/filebeat-failed-to-publish-events-caused-by-client-is-not-connected/217603/6

I'm embarrassed it took me so long to solve this. Turns out all I needed to do was specify a client_inactivity_timeout of more than the default of 60 seconds. In my particular situation the client (on a test server) was relatively inactive, causing Logstash to kill the connection after 60 seconds. Increasing this beyond the inactivity time resolved the issue. input { beats { client_inactivity_timeout => 1200 port => 5044 } } Thanks, Greg。

> `Failed to execute action {:id=>:main, :action_type=>LogStash::ConvergeResult::FailedAction, :message=>"Could not execute action: PipelineAction::Create<main>, action_result: false", :backtrace=>nil}`
>
> `Logstash stopped processing because of an error: (SystemExit) exit`



> ` Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}`

it is an informational message.

## 部署问题排查

### 数据流向

1. 源头数据(xxx.log)
2. 数据抽取(Filebeat)
3. 数据解析(Logstash)
4. 数据存储(Elasticsearch)
5. 数据展示(search-center,log-center)

### 数据抽取

1. 检查filebeat的配置文件



### 数据解析

## reference

- 官网：https://www.elastic.co/cn/what-is/elk-stack
- Elasticsearch：https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html
- Elasticsearch2.X：https://github.com/elastic/elasticsearch-definitive-guide/
- Filebeat：https://www.elastic.co/cn/products/beats/filebeat
- 部署参考：https://blog.csdn.net/m0_37063785/article/details/101074010
- docker官方：https://elk-docker.readthedocs.io/



